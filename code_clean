import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv("D:\\Projects\\Pricing Analyst Project\\crop_price_prediction\\data from code\\canola_price_prediction.csv")

# Define features (X) and target (y)
X = df.drop(columns=["Mean Price"])
y = df["Mean Price"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Scale the data (only for Linear Regression)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Function to evaluate model performance
def evaluate_model(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    
    return mae, mse, rmse, r2

# Linear Regression (no hyperparameter tuning)
linear_model = LinearRegression()
linear_results = evaluate_model(linear_model, X_train_scaled, X_test_scaled, y_train, y_test)

# Random Forest with Hyperparameter Tuning
rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
rf_model = RandomForestRegressor(random_state=42)
rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=2)
rf_grid_search.fit(X_train, y_train)
best_rf = rf_grid_search.best_estimator_
rf_results = evaluate_model(best_rf, X_train, X_test, y_train, y_test)

# XGBoost with Hyperparameter Tuning
xgb_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'reg_lambda': [0.1, 1, 10],
    'reg_alpha': [0.1, 1, 10]
}
xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)
xgb_random_search = RandomizedSearchCV(xgb_model, xgb_param_grid, n_iter=50, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=2, random_state=42)
xgb_random_search.fit(X_train, y_train)
best_xgb = xgb_random_search.best_estimator_
xgb_results = evaluate_model(best_xgb, X_train, X_test, y_train, y_test)

# Print results
results = pd.DataFrame({
    "Model": ["Linear Regression", "Random Forest", "XGBoost"],
    "MAE": [linear_results[0], rf_results[0], xgb_results[0]],
    "MSE": [linear_results[1], rf_results[1], xgb_results[1]],
    "RMSE": [linear_results[2], rf_results[2], xgb_results[2]],
    "RÂ²": [linear_results[3], rf_results[3], xgb_results[3]]
})

print(results)
